"""
Execute query plans generated by Gemini AI.
Validates against FieldCatalog and executes Django ORM queries.
"""
import json
import re
from decimal import Decimal
from datetime import date, datetime
from typing import Dict, List, Any, Optional
from django.db.models import Sum, Count, Avg, Max, Min, Q
from django.db.models.functions import ExtractYear, ExtractMonth

from analytics.models import FieldCatalog
from core.models import Organization
from analytics.models import (
    MarketingCampaign, LocationDemandMonthly, OrgKPI_Daily,
    ProjectKPI_Daily, OrgMonthlySnapshot, ProjectMonthlySnapshot
)
from crm.models import Customer, Booking
from projects.models import Project, Unit
from core.models import Employee


# Dataset to Django model mapping
DATASET_MODEL_MAP = {
    "marketing_campaign": MarketingCampaign,
    "location_demand": LocationDemandMonthly,
    "org_kpi": OrgKPI_Daily,
    "project_kpi": ProjectKPI_Daily,
    "org_snapshot": OrgMonthlySnapshot,
    "project_snapshot": ProjectMonthlySnapshot,
    "customer": Customer,
    "booking": Booking,
    "project": Project,
    "unit": Unit,
    "employee": Employee,
}


def extract_json(text: str) -> Optional[Dict]:
    """Extract first JSON object from text."""
    match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            pass
    return None


def get_field_catalog(dataset: str) -> Dict[str, FieldCatalog]:
    """Get enabled fields for a dataset."""
    fields = FieldCatalog.objects.filter(dataset=dataset, is_enabled=True)
    return {f.key: f for f in fields}


def validate_field(field_key: str, dataset: str) -> Optional[FieldCatalog]:
    """Validate field exists and is enabled for dataset."""
    # FieldCatalog keys are formatted as "dataset.field_name"
    full_key = f"{dataset}.{field_key}"
    try:
        field = FieldCatalog.objects.get(key=full_key, dataset=dataset, is_enabled=True)
        return field
    except FieldCatalog.DoesNotExist:
        # Fallback: try just field_key (backwards compatibility)
        try:
            return FieldCatalog.objects.get(key=field_key, dataset=dataset, is_enabled=True)
        except FieldCatalog.DoesNotExist:
            # Try to create a dynamic field mapping based on model introspection
            if dataset in DATASET_MODEL_MAP:
                model_class = DATASET_MODEL_MAP[dataset]
                if hasattr(model_class, '_meta'):
                    field_names = [f.name for f in model_class._meta.get_fields() if hasattr(f, 'name')]
                    if field_key in field_names:
                        # Create a temporary field object for direct model fields
                        class TempField:
                            def __init__(self, key, orm_path, data_type):
                                self.key = key
                                self.orm_path = orm_path
                                self.data_type = data_type
                                self.label = key.replace('_', ' ').title()

                        # Determine data type
                        from django.db import models as django_models
                        try:
                            model_field = model_class._meta.get_field(field_key)
                            if isinstance(model_field, (django_models.DecimalField, django_models.FloatField)):
                                data_type = "decimal"
                            elif isinstance(model_field, (django_models.IntegerField, django_models.BigIntegerField)):
                                data_type = "integer"
                            elif isinstance(model_field, (django_models.CharField, django_models.TextField)):
                                data_type = "string"
                            else:
                                data_type = "string"
                            return TempField(field_key, field_key, data_type)
                        except:
                            pass
            return None


def build_queryset(model_class, org: Organization, filters: Dict, dimensions: List[str], metrics: List[str], dataset: str):
    """Build Django queryset with org filter, dimensions, and metrics."""
    # Apply org filter based on model structure
    if hasattr(model_class, '_meta') and 'organization' in [f.name for f in model_class._meta.get_fields()]:
        qs = model_class.objects.filter(organization=org)
    elif dataset == "booking":
        # Booking -> customer -> organization
        qs = model_class.objects.filter(customer__organization=org)
    elif dataset == "unit":
        # Unit -> project -> organization
        qs = model_class.objects.filter(project__organization=org)
    elif dataset == "project_kpi":
        # ProjectKPI_Daily -> project -> organization
        qs = model_class.objects.filter(project__organization=org)
    elif dataset == "project_snapshot":
        qs = model_class.objects.filter(organization=org)
    else:
        # Fallback: try organization filter
        qs = model_class.objects.filter(organization=org)
    
    # Apply filters
    for filter_key, filter_value in filters.items():
        if filter_key == "organization":
            continue  # Already filtered
        field = validate_field(filter_key, dataset)
        if field:
            lookup = field.orm_path
            if field.data_type == "date":
                if isinstance(filter_value, str):
                    try:
                        filter_value = datetime.fromisoformat(filter_value.replace('Z', '+00:00')).date()
                    except:
                        pass
            qs = qs.filter(**{lookup: filter_value})
    
    return qs


def aggregate_metrics(qs, metrics: List[str], dataset: str):
    """Apply aggregations to queryset."""
    agg_dict = {}
    for metric_key in metrics:
        field = validate_field(metric_key, dataset)
        if not field:
            continue
        orm_path = field.orm_path
        if field.data_type in ("decimal", "integer"):
            agg_dict[f"{metric_key}_sum"] = Sum(orm_path)
            agg_dict[f"{metric_key}_avg"] = Avg(orm_path)
            agg_dict[f"{metric_key}_max"] = Max(orm_path)
            agg_dict[f"{metric_key}_min"] = Min(orm_path)
        elif field.data_type == "string":
            agg_dict[f"{metric_key}_count"] = Count(orm_path)
    
    if not agg_dict:
        return {}
    
    result = qs.aggregate(**agg_dict)
    # Clean up: use sum for numeric, count for string
    cleaned = {}
    for metric_key in metrics:
        field = validate_field(metric_key, dataset)
        if not field:
            continue
        if field.data_type in ("decimal", "integer"):
            cleaned[metric_key] = float(result.get(f"{metric_key}_sum", 0) or 0)
        elif field.data_type == "string":
            cleaned[metric_key] = result.get(f"{metric_key}_count", 0)
    
    return cleaned


def group_by_dimensions(qs, dimensions: List[str], metrics: List[str], dataset: str, limit: int = 50):
    """Group by dimensions and aggregate metrics."""
    if not dimensions:
        return []
    
    # Validate dimensions
    dim_fields = []
    for dim_key in dimensions:
        field = validate_field(dim_key, dataset)
        if field:
            dim_fields.append((dim_key, field.orm_path))
    
    if not dim_fields:
        return []
    
    # Build values() and annotate()
    values_list = [field.orm_path for _, field.orm_path in dim_fields]
    
    # Add metric aggregations
    annotate_dict = {}
    for metric_key in metrics:
        field = validate_field(metric_key, dataset)
        if not field:
            continue
        orm_path = field.orm_path
        if field.data_type in ("decimal", "integer"):
            annotate_dict[f"{metric_key}_sum"] = Sum(orm_path)
        elif field.data_type == "string":
            annotate_dict[f"{metric_key}_count"] = Count(orm_path)
    
    # If no metrics provided, add a count
    if not annotate_dict:
        annotate_dict["count"] = Count("id")
    
    grouped = qs.values(*values_list).annotate(**annotate_dict).order_by(f"-{list(annotate_dict.keys())[0] if annotate_dict else values_list[0]}")[:limit]
    
    # Format results
    results = []
    for row in grouped:
        result_row = {}
        for dim_key, orm_path in dim_fields:
            result_row[dim_key] = row.get(orm_path, "")
        if metrics:
            for metric_key in metrics:
                field = validate_field(metric_key, dataset)
                if not field:
                    continue
                if field.data_type in ("decimal", "integer"):
                    result_row[metric_key] = float(row.get(f"{metric_key}_sum", 0) or 0)
                elif field.data_type == "string":
                    result_row[metric_key] = row.get(f"{metric_key}_count", 0)
        else:
            # Use count when no metrics provided
            result_row["count"] = row.get("count", 0)
        results.append(result_row)
    
    return results


def run_plan(plan: Dict, org: Organization) -> Dict[str, Any]:
    """
    Execute a query plan and return chart/table data.
    
    Args:
        plan: JSON plan from Gemini with dataset, metrics, dimensions, filters, chart_type, etc.
        org: Organization to scope queries to
    
    Returns:
        Dict with chart, table, answer, and metadata
    """
    dataset = plan.get("dataset", "")
    if dataset not in DATASET_MODEL_MAP:
        return {
            "error": f"Unknown dataset: {dataset}",
            "chart": None,
            "table": None,
            "answer": f"Dataset '{dataset}' is not available.",
        }
    
    model_class = DATASET_MODEL_MAP[dataset]
    filters = plan.get("filters", {})
    dimensions = plan.get("dimensions", [])
    metrics = plan.get("metrics", [])
    chart_type = plan.get("chart_type", "table")
    limit = min(plan.get("limit", 50), 50)  # Cap at 50
    
    # Safety check: If dimensions exist but chart_type is "table" or "answer", default to "pie"
    # This handles cases where the prompt correction didn't work
    if dimensions and len(dimensions) > 0 and chart_type in ("table", "answer"):
        chart_type = "pie"
        print(f"DEBUG executor: Overriding chart_type to 'pie' (dimensions exist but chart_type was {plan.get('chart_type')})")
    
    # Build queryset
    qs = build_queryset(model_class, org, filters, dimensions, metrics, dataset)
    
    # If no dimensions, return simple aggregation
    if not dimensions:
        if not metrics:
            # Fallback: count records
            count = qs.count()
            # Generate a better answer based on dataset
            if dataset == "employee":
                answer = f"There are {count} employee{'s' if count != 1 else ''} in your organization."
            elif dataset == "customer":
                answer = f"You have {count} customer{'s' if count != 1 else ''}."
            elif dataset == "project":
                answer = f"You have {count} project{'s' if count != 1 else ''}."
            elif dataset == "booking":
                answer = f"You have {count} booking{'s' if count != 1 else ''}."
            else:
                answer = f"Total records: {count}"
            return {
                "chart": None,
                "table": {"columns": ["Count"], "rows": [[count]]},
                "answer": answer,
            }
        
        # Aggregate metrics
        agg_result = aggregate_metrics(qs, metrics, dataset)
        if not agg_result:
            return {
                "chart": None,
                "table": {"columns": ["No data"], "rows": [["No matching records"]]},
                "answer": "No data found.",
            }
        
        # Format as simple answer
        answer_parts = []
        for metric_key, value in agg_result.items():
            field = validate_field(metric_key, dataset)
            label = field.label if field else metric_key
            if isinstance(value, (int, float)) and value >= 1000:
                answer_parts.append(f"{label}: â‚¹{value:,.0f}")
            else:
                answer_parts.append(f"{label}: {value}")
        
        return {
            "chart": None,
            "table": {
                "columns": list(agg_result.keys()),
                "rows": [[v for v in agg_result.values()]],
            },
            "answer": ". ".join(answer_parts),
        }
    
    # Group by dimensions
    grouped_data = group_by_dimensions(qs, dimensions, metrics, dataset, limit)
    
    if not grouped_data:
        return {
            "chart": None,
            "table": {"columns": ["No data"], "rows": [["No matching records"]]},
            "answer": "No data found.",
        }
    
    # Build table
    # Use count if no metrics provided
    effective_metrics = metrics if metrics else ["count"]
    table_columns = dimensions + effective_metrics
    table_rows = []
    for row in grouped_data:
        table_rows.append([row.get(col, "") for col in table_columns])
    
    # Build chart data based on chart_type
    chart_data = None
    print(f"DEBUG executor: chart_type={chart_type}, dimensions={dimensions}, grouped_data length={len(grouped_data)}")
    if chart_type in ("bar", "line", "pie"):
        print(f"DEBUG executor: Chart type is valid: {chart_type}")
        if len(dimensions) >= 1 and len(grouped_data) > 0:
            print(f"DEBUG executor: Conditions met, generating chart")
            # Use first dimension and first metric (or "count" if no metrics)
            dim_key = dimensions[0]
            metric_key = metrics[0] if metrics else "count"
            
            # Generate chart if we have data (field validation is nice-to-have but not required)
            chart_data = {
                "type": chart_type,
                "x": dim_key,
                "y": metric_key,
                "data": grouped_data,
            }
            print(f"DEBUG executor: Chart data generated: {chart_data}")
        else:
            print(f"DEBUG executor: Conditions NOT met - dimensions={len(dimensions)}, grouped_data={len(grouped_data)}")
    else:
        print(f"DEBUG executor: Chart type NOT in valid types: {chart_type}")
    
    return {
        "chart": chart_data,
        "table": {
            "columns": table_columns,
            "rows": table_rows,
        },
        "answer": f"Found {len(grouped_data)} records.",
    }
